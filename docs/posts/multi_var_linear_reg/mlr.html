
<!DOCTYPE html>
<html>

  <head>
  <link rel="apple-touch-icon" sizes="180x180" href="../../images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../images/favicon-16x16.png">
  <link rel="manifest" href="../../images/site.webmanifest">
  <link rel="mask-icon" href="../../images/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="../../images/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-config" content="/assets/logos/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141821189-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141821189-1');
</script>

  
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Univariety Linear Regression</title>
  <meta name="description" content="Cross-posted from Microsoft Research Blog">

  
  
  <link rel="stylesheet" href="https://decentdescent.org/assets/style.css">

  <link rel="canonical" href="https://decentdescent.org/tp5.html">
  <link rel="alternate" type="application/rss+xml" title="Decent Descent" href="https://decentdescent.org/feed.xml">

  <script async defer src="https://buttons.github.io/buttons.js"></script>


  <!-- mathjax -->
  <!-- <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script> -->
  <!-- <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
	jax: ["input/TeX","output/HTML-CSS"],
	displayAlign: "left",
	displayIndent: "2em"
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script> -->
  <!-- mathjax v3 -->
<!--
  <script type="text/javascript">
  document.addEventListener('DOMContentLoaded', function(){
    function stripcdata(x) {
      if (x.startsWith('% <![CDATA[') && x.endsWith('%]]>'))
	return x.substring(11,x.length-4);
      return x;
    }
    document.querySelectorAll("script[type='math/tex']").forEach(function(el){
      el.outerHTML = "\\(" + stripcdata(el.textContent) + "\\)";
    });
    document.querySelectorAll("script[type='math/tex; mode=display']").forEach(function(el){
      el.outerHTML = "\\[" + stripcdata(el.textContent) + "\\]";
    });
    var script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js";
    document.head.appendChild(script);
  }, false);
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
-->
  <!-- Load jQuery -->
  <!--
  <script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
  -->
  <!-- Load KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <!--
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
  -->

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <!--
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
  -->

  <style>
    body, html {
        margin: 0;
        padding: 0;
        height: 100%;
        width: 100%;
        font-size: 16px;
    }
    code {
        color: #000000;
        font-size: 16px;
    }
    pre {
        margin: 0;
        padding: 20px;
        box-sizing: border-box;
        height: 100%;
        width: 100%;
        overflow: auto;
        background-color: #eeeeee;


        font-size: 16px;
        line-height: 1.5;
        font-family: Consolas, "Courier New", monospace;
    }
</style>

</head>


  <body>

    <header class="px-2 clearfix">
  <div class="left sm-width-full py-1 mt-1 mt-lg-0">
    <!--
    <a class="align-middle link-primary text-accent" href="/">
      Decent Descent
    </a>
    -->
    <a class="align-middle link-primary mt-1" href="/">
        <img src="../../images/decent_descent_abbrev_black_text2path.svg" height="36" width="auto">
    </a>
  </div>
  <div class="right sm-width-full">
    <ul class="list-reset mt-lg-1 mb-2 mb-lg-1">
      
        
      
        
        <li class="inline-block">
          <a class="align-middle link-primary mr-2 mr-lg-0 ml-lg-2" href="../../about.html">
            About
          </a>
        </li>
        
      
        
 
    </ul>
  </div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css" integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
</header>


    <div>
      

<article class="container px-2 mx-auto mb4" itemscope itemtype="http://schema.org/BlogPosting">
  
  <h1 class="h0 col-9 sm-width-full py-0 mt-7 mb-0 inline-block" itemprop="name headline">Multivariate Linear Regression</h1>
  <h1 class="h1 col-9 sm-width-full py-0 mt-0 mb-4 inline-block" itemprop="name headline">C Programming</h1>
  
  <div class="col-12 sm-width-full mt-1 border-top-thin ">
    <p class="text-accent mb-0 pt-2 pb-0 bold authordate"><a class='link-authordate' href="https://x.com/oderoi_">Isack Odero</a> 
    </p>
    <p class="mb-1 pt-0 pb-0 bold authordate"><time datetime="2022-03-08T14:00:00+00:00" itemprop="datePublished">Aug 27, 2024</time>
    
  <div class="table">
    <!-- <div class="inline-block mb-0 mr-1">
      <a href="https://arxiv.org/abs/2203.03466" title="Read the Paper" class="link-social block">
  <svg height="32" class="header-social-op" version="1.1" width="32" viewBox="0 0 16 16" aria-hidden="true" fill-rule="evenodd">
    <path d="M8,0C3.6,0,0,3.6,0,8s3.6,8,8,8s8-3.6,8-8S12.4,0,8,0z 
             M9.08 2.6H4.76c-.595 0-1.08.485-1.08 1.08v8.64c0 .595.485 1.08 1.08 1.08h6.48c.595 0 1.08-.485 1.08-1.08V5.84zm2.43 3.78H8.54V3.41z"/>
  </svg>
</a>

    </div> -->
    <div class="inline-block mb-0">
      <a href="https://github.com/oderoi/C/tree/main/Algoritm/Artificial%20Intelligence/Univariate%20Linear%20Regression" title="Clone the Code" class="link-social block">
<svg height="32" class="octicon octicon-mark-github header-social-op" viewBox="0 0 16 16" version="1.1" width="32" aria-hidden="true"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
</svg>
</a>

    </div>
  </div>
    
    </p>
  </div>

  <div class="prose" itemprop="articleBody">
      <!-- <p><em>Cross-posted from <a href="https://www.microsoft.com/en-us/research/blog/%c2%b5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/">Microsoft Research Blog</a></em></p> -->

<!-- ![An animated line-plot showing the stability of optimal learning rate as we change the neural network's parametrization. The parametrization is varied by interpolating between mup-Parametrization and PyTorch default in terms of the scaling for the learning rate and the initialization scale. The animation shows that mu-Parametrization is the only parametrization that preserves the optimality of learning rate across model widths; it also achieves the best absolute performance across all parametrizations.](https://www.microsoft.com/en-us/research/uploads/prod/2022/03/1400x788_Hyperparameters_no_logo_hero.gif) -->

<!-- <p><img src="/assets/tp5/anim_notitle_verbose.gif" class="limit-height" style="max-height:300px;" /></p> -->

<h2>defined</h2>
<p>Multivariate linear regression is a machine learning technique used to analyze the relationship between multiple independent variables and multiple dependent variables simultaneously. Unlike multiple linear regression, which focuses on predicting a single dependent variable from multiple predictors, multivariate linear regression allows for the modeling of multiple correlated outcome variables.</p>

<h2>1.0 Problem statement</h2>

<p>We will use the motivating example of housing price prediction. The training dataset contains four features (size, bedrooms, floors and, age) shown in the table below.</p>


<table>
  <tr>
    <th>Size (sqf) </th>
    <th>Number of bedrooms</th>
    <th>Number of floors</th>
    <th>Age of Home</th>
    <th>Price (1000$)</th>
    
</tr>
  <tr>
    <td>2104</td>
    <td>5</td>
    <td>1</td>
    <td>45</td>
    <td>460</td>
  </tr>
  <tr>
    <td>1416</td>
    <td>3</td>
    <td>2</td>
    <td>40</td>
    <td>232</td>
  </tr>
  <tr>
    <td>852</td>
    <td>2</td>
    <td>1</td>
    <td>35</td>
    <td>178</td>
  </tr>
</table>
<p>Let's build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old. </p>
<!-- [Read the paper](https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/)

[Download the code](http://github.com/microsoft/mup) -->

<h3>1.1 Matrix \(X\) containing our example</h3>
<p>Each row of datasets represent one example, with $n$ number of features and $m$ training examples, now \(\mathbf{x}\) is an input matrix with dimensions \((m, n)\) where $m$ is row and \(n\) is column.</p>
<!-- [![Two line-plots showing the change in activation scale between PyTorch default and the µ-Parametrization. Under PyTorch default, the activation scale grows as the network width increases for a particular time step. Under µ-Parametrization, the activation scale is stable across widths for a particular time step. ](https://www.microsoft.com/en-us/research/uploads/prod/2022/03/logits_attnlogits_embedding_edited.jpg)](https://www.microsoft.com/en-us/research/uploads/prod/2022/03/logits_attnlogits_embedding_edited.jpg) -->

$$\mathbf{X} = 
\begin{pmatrix}
 x^{(0)}_0 & x^{(0)}_1 & \cdots & x^{(0)}_{n-1} \\ 
 x^{(1)}_0 & x^{(1)}_1 & \cdots & x^{(1)}_{n-1} \\
 \vdots  &  \vdots  &  \vdots  &  \vdots  \\
 x^{(m-1)}_0 & x^{(m-1)}_1 & \cdots & x^{(m-1)}_{n-1} 
\end{pmatrix}
$$

<p>notation:</p>
<ul>
    <li>\(\mathbf{x}^{(i)}\) is vector containing example i. \(\mathbf{x}^{(i)}\) \( = (x^{(i)}_0, x^{(i)}_1, \cdots,x^{(i)}_{n-1})\)</li>
    <li>\(x^{(i)}_j\) is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element. </li>
</ul>

<h3>1.2 Parameter vector \(\mathbf{w}_{j}\) , b</h3>
<ul>
    <li>
        \(\mathbf{w}\) is a vector with \(n\) elements.
        <ul>
            <li>
                Each element contains the parameter associated with one feature.
            </li>
            <li>
                notionally, we draw this as a column vector
            </li>
            $$\mathbf{w} = \begin{pmatrix}
            w_0 \\ 
            w_1 \\
            \vdots\\
            w_{n-1}
            \end{pmatrix}
            $$
        </ul>
    </li>
    <li>\(b\) is a scalar parameter. </li>
</ul>

<h2>2.0 Load datasets</h2>
<p>As shown in a problem statement, our dataset contains five features (size, bedrooms, floors and, age, price of the house), \(n = 5\) and ahundred examples, \(m = 100\).</p>
<p>In this case our dataset will have the size of \((m , n) = (100 , 5)\)</p>

$$\mathbf{data} = 
\begin{pmatrix}
 x^{(0)}_{0} & x^{(0)}_{1} & x^{(0)}_{2} & \cdots & y^{(0)}_{n-1} \\ 
 x^{(1)}_{0} & x^{(1)}_{1} & x^{(1)}_{2} & \cdots & y^{(1)}_{n-1} \\
 \vdots      & \vdots      & \vdots      & \vdots      & \vdots\\
 x^{(m-1)}_{0} & x^{(m-1)}_{1} & x^{(m-1)}_{2} & \cdots & y^{(m-1)}_{n-1}
\end{pmatrix}

=
 
\begin{pmatrix}
 x^{(0)}_{0} & x^{(0)}_{1} & x^{(0)}_{2} & x^{(0)}_{3} & y^{(0)}_{4} \\ 
 x^{(1)}_{0} & x^{(1)}_{1} & x^{(1)}_{2} & x^{(1)}_{3} & y^{(1)}_{4} \\
 \vdots      & \vdots      & \vdots      & \vdots      & \vdots\\
 x^{(99)}_{0} & x^{(99)}_{1} & x^{(99)}_{2} & x^{(99)}_{3} & y^{(99)}_{4}
\end{pmatrix}
$$

$$data \in \mathbb{R}_{m \times n}  =  \mathbb{R}_{100 \times 5}$$

<h3>2.1 Load input datasets, \(\mathbf{x}^{(i)}_{j}\)</h3>

<p>Input datasets will comprise of five features (size, bedrooms, floors and, age), $n = 4$ and hundred exaples,\(m = 100\).</p>

$$\mathbf{X} = 
\begin{pmatrix}
 x^{(0)}_{0} & x^{(0)}_{1} & \cdots & x^{(0)}_{n-1}  \\ 
 x^{(1)}_{0} & x^{(1)}_{1} & \cdots & x^{(1)}_{n-1}  \\
 \vdots      & \vdots      & \vdots      & \vdots \\
 x^{(m-1)}_{0} & x^{(m-1)}_{1} & \cdots & x^{(m-1)}_{n-1} 
\end{pmatrix}
 = 
\begin{pmatrix}
 x^{(0)}_{0} & x^{(0)}_{1} & x^{(0)}_{2} & x^{(0)}_{3}  \\ 
 x^{(1)}_{0} & x^{(1)}_{1} & x^{(1)}_{2} & x^{(1)}_{3}  \\
 \vdots      & \vdots      & \vdots      & \vdots \\
 x^{(99)}_{0} & x^{(99)}_{1} & x^{(99)}_{2} & x^{(99)}_{3} 
\end{pmatrix}
$$

$$\mathbf{X} \in \mathbb{R}_{m \times n}  =  \mathbb{R}_{100 \times 4}$$

<h3>2.2 Load output datasets, \(\mathbf{y}^{(i)}_{j}\)</h3>

<p>Input datasets will comprise of only one features (price), \(n = 1\) and hundred exaples,\(m = 100\).</p>


$$\mathbf{Y} = 
\begin{pmatrix}
  y^{(0)}_{0} \\ 
  y^{(1)}_{0} \\
  \vdots     \\
 y^{(m-1)}_{0}
\end{pmatrix}
 = 
\begin{pmatrix}
  y^{(0)}_{0} \\ 
  y^{(1)}_{0} \\
  \vdots     \\
 y^{(99)}_{0}
\end{pmatrix}
$$

$$Y \in \mathbb{R}_{m \times n}  =  \mathbb{R}_{100 \times 1}$$

<h3>2.3 Initialize parameters  \(\mathbf{w}_{j}  , b\)</h3>

<ul>
    <li>
        \(\mathbf{w}_{j}\) is a vector with $n = 4$ elements.

        <ul>
            <li>
                Each element contains the parameter associated with one feature.
            </li>
            $$\mathbf{w} = \begin{pmatrix}
            w_{0} \\ 
            w_{1} \\
            \vdots\\
            w_{n-1}
            \end{pmatrix}
            = \begin{pmatrix}
            w_{0} \\ 
            w_{1} \\
            w_{2}\\
            w_{3}
            \end{pmatrix}
            $$

            $$\mathbf{w} \in \mathbb{R}_{n \times 1}  =  \mathbb{R}_{4 \times 1}$$
        </ul>
    </li>
    <li>\(b\) is a scalar parameter.</li>
</ul>

<h2> 3.0 Model Prediction With Multiple Variables</h2>
<p>The model's prediction with multiple variables is given by the linear model:</p>

$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \tag{1}$$
or in vector notation:
$$ f_{\mathbf{w},b}(\mathbf{x}) = \mathbf{w} \cdot \mathbf{X} + \mathbf{b}  \tag{2} $$ 

$$f_{\mathbf{w},b}(\mathbf{x}) \in \mathbb{R}_{m \times 1}  =  \mathbf{w} \in \mathbb{R}_{n \times 1} \cdot \mathbf{X} \in \mathbb{R}_{m \times n} + \mathbf{b} \in \mathbb{R}_{1}$$

$$f_{\mathbf{w},b}(\mathbf{x})_{m \times 1}  =  \mathbf{w}_{n \times 1} \cdot \mathbf{X}_{m \times n} + \mathbf{b}_{1}$$

$$f_{\mathbf{w},b}(\mathbf{x}^{(i)}) \in \mathbb{R}_{m \times 1}$$

$$f_{\mathbf{w},b}(\mathbf{x}) = 
\begin{pmatrix}
  f_{\mathbf{w},b}(\mathbf{x})^{(0)}_{0} \\ 
  f_{\mathbf{w},b}(\mathbf{x})^{(1)}_{0} \\
  \vdots     \\
 f_{\mathbf{w},b}(\mathbf{x})^{(m-1)}_{0}
\end{pmatrix}
 = 
\begin{pmatrix}
  f_{\mathbf{w},b}(\mathbf{x})^{(0)}_{0} \\ 
  f_{\mathbf{w},b}(\mathbf{x})^{(1)}_{0} \\
  \vdots     \\
 f_{\mathbf{w},b}(\mathbf{x})^{(99)}_{0}
\end{pmatrix}
$$
<p>where \(\cdot\) is a vector <it>dot product</it></p>

<p>To demonstrate the dot product, we will implement prediction using (1) and (2).</p>

<h2>4.0 Compute Cost With Multiple Variables</h2>
<p>The equation for the cost function with multiple variables \(J(\mathbf{w},b)\) is:</p>
$$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 \tag{3}$$ 
where:
$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{X} + b  \tag{4} $$ 

$$f_{\mathbf{w},b}(\mathbf{x}) \in \mathbb{R}_{m \times 1}$$

<p>In contrast to previous labs, \(\mathbf{w}\) and \(\mathbf{x}^{(i)}\) are vectors rather than scalars supporting multiple features.</p>

<h2>5.0 Gradient Descent With Multiple Variables</h2>
<p>Gradient descent for multiple variables:</p>
$$\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \newline\;
& w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{5}  \; & \text{for j = 0..n-1}\newline
&b\ \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline \rbrace
\end{align*}$$

<p>where, n is the number of features, parameters \(w_j\),  $b$, are updated simultaneously and where  </p>

$$
\begin{align}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \tag{6}  \\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{7}
\end{align}
$$

<ul>
    <li>m is the number of training examples in the data set</li>
    <li>\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\) is the model's prediction, while \(y^{(i)}\) is the target value</li>
</ul>
<p>Note:</p>
<ul>
    <li>$$\frac{\partial J(\mathbf{w},b)}{\partial w_j} \in \mathbb{R}_{1 \times n} = \mathbb{R}_{1 \times 4}$$</li>
    $$\frac{\partial J(\mathbf{w},b)}{\partial w_j}
= 
\begin{pmatrix}
  \frac{\partial J(\mathbf{w},b)}{\partial w_0} ,
  \frac{\partial J(\mathbf{w},b)}{\partial w_1} ,
  \cdots ,
 \frac{\partial J(\mathbf{w},b)}{\partial w_{n-1}}
\end{pmatrix}
= 
\begin{pmatrix}
  \frac{\partial J(\mathbf{w},b)}{\partial w_0} ,
  \frac{\partial J(\mathbf{w},b)}{\partial w_1} ,
  \frac{\partial J(\mathbf{w},b)}{\partial w_2} ,
 \frac{\partial J(\mathbf{w},b)}{\partial w_3}
\end{pmatrix}
$$
</ul>





<p>continue ...</p>


  </div>
  <div class="col-4 sm-width-full mt-3 border-top-thin "/>
  <div class="table pt-2">
    <div class="inline-block mb-0 mr-1">
      <svg height="32" class="header-social-accent" version="1.1" width="32" viewBox="0 0 16 16" aria-hidden="true">
 <circle cx="8" cy="8" r="8"/>
</svg>

    </div>
    <div class="inline-block mb-0 mr-1">
      <svg height="32" class="header-social-accent" version="1.1" width="32" viewBox="0 0 16 16" aria-hidden="true">
 <circle cx="8" cy="8" r="8"/>
</svg>

    </div>
    <div class="inline-block mb-0 mr-1">
      <a href="https://twitter.com/intent/tweet?text=Univariety+Linear+Regression&amp;url=https%3A%2F%2Foderoi.github.io%2Fposts%2Funi_linear_reg%2Funi_linear_reg.html" title="Share on twitter" class="link-social block">
  <svg height="32" class="header-social-op" version="1.1" width="32" viewBox="0 0 16 16" aria-hidden="true">
    <path d="M8,0C3.6,0,0,3.6,0,8s3.6,8,8,8s8-3.6,8-8S12.4,0,8,0z M12,6c0,0.1,0,0.2,0,0.3c0,2.7-2.1,5.8-5.8,5.8
  	c-1.2,0-2.2-0.3-3.1-0.9c0.2,0,0.3,0,0.5,0c1,0,1.8-0.3,2.5-0.9c-0.9,0-1.7-0.6-1.9-1.4c0.1,0,0.3,0,0.4,0c0.2,0,0.4,0,0.5-0.1
  	c-0.9-0.2-1.6-1-1.6-2v0C3.7,6.9,4,7,4.3,7.1c-0.5-0.4-0.9-1-0.9-1.7c0-0.4,0.1-0.7,0.3-1c1,1.2,2.5,2.1,4.2,2.1
  	c0-0.2-0.1-0.3-0.1-0.5c0-1.1,0.9-2,2.1-2c0.6,0,1.1,0.2,1.5,0.6c0.5-0.1,0.9-0.3,1.3-0.5c-0.2,0.5-0.5,0.9-0.9,1.1
  	c0.4,0,0.8-0.2,1.2-0.3h0C12.7,5.3,12.4,5.7,12,6z"/>
  </svg>
</a>

    </div>

    <div class="inline-block mb-0 mr-1">
      <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Foderoi.github.io%2Fposts%2Funi_linear_reg%2Funi_linear_reg.html&title=Univariety+Linear+Regression" title="Share on LinkedIn" class="link-social block">
        <svg fill="#000000" height="32" width="32" aria-hidden="true" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="-143 145 512 512" xml:space="preserve"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path d="M113,145c-141.4,0-256,114.6-256,256s114.6,256,256,256s256-114.6,256-256S254.4,145,113,145z M41.4,508.1H-8.5V348.4h49.9 V508.1z M15.1,328.4h-0.4c-18.1,0-29.8-12.2-29.8-27.7c0-15.8,12.1-27.7,30.5-27.7c18.4,0,29.7,11.9,30.1,27.7 C45.6,316.1,33.9,328.4,15.1,328.4z M241,508.1h-56.6v-82.6c0-21.6-8.8-36.4-28.3-36.4c-14.9,0-23.2,10-27,19.6 c-1.4,3.4-1.2,8.2-1.2,13.1v86.3H71.8c0,0,0.7-146.4,0-159.7h56.1v25.1c3.3-11,21.2-26.6,49.8-26.6c35.5,0,63.3,23,63.3,72.4V508.1z "></path> </g></svg>
      </a>
    </div>
    
    <!-- <div class="inline-block mb-0 mr-1">
      <a onclick="parent.postMessage('submit','*')" href="https://news.ycombinator.com/submitlink?u=https%3A%2F%2Fdecentdescent.org%2Ftp5.html&amp;t=Tuning+GPT-3+on+a+Single+GPU" title="Share on Hacker News" class="link-social block">
  <svg height="32" class="header-social-op" version="1.1" width="32" viewBox="0 0 496 496" aria-hidden="true">
    <path d="M 248,0 C 111,0 0,111 0,248 0,385 111,496 248,496 385,496 496,385 496,248 496,111 385,0 248,0 Z m -96,120 h 37.30078 c 52.5,98.3 49.19883,101.19961 59.29883,125.59961 12.3,-27 5.79961,-24.39961 60.59961,-125.59961 H 344 L 263.19922,275.09961 V 376 H 231.80078 V 273.30078 Z M 45.087891,221.17773 c 0.03739,0.007 0.07799,0.0215 0.111328,0.0215 H 45 c 0.01711,-0.0286 0.0505,-0.0286 0.08789,-0.0215 z
"/>
  </svg>
</a>

    </div> -->
    <!-- <div class="inline-block mb-0 mr-1">
      <a href="http://reddit.com/submit?url=https%3A%2F%2Fdecentdescent.org%2Ftp5.html&amp;title=Tuning+GPT-3+on+a+Single+GPU" title="Share on Reddit" class="link-social block">
  <svg height="32" class="header-social-op" version="1.1" width="32" viewBox="8 8 496 496" aria-hidden="true">
    <path d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5
    26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/>
  </svg>
</a>

    </div> -->
    <!-- <div class="inline-block mb-0 mr-1">
      <a href="https://facebook.com/sharer.php?u=https%3A%2F%2Fdecentdescent.org%2Ftp5.html" title="Share on Facebook" class="link-social block">
  <svg height="32" class="header-social-op" version="1.1" width="32" viewBox="8 8 496 496" aria-hidden="true">
    <path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"
/>
  </svg>
</a>

    </div> -->
  </div>

</article>


    </div>

    <!--
<div class="border-top-thin clearfix mt-2 mt-lg-4">
  <div class="container mx-auto px-2">
    <p class="col-8 sm-width-full left py-2 mb-0">This project is maintained by <a class="text-accent" href="https://github.com/"></a></p>
    <ul class="list-reset right clearfix sm-width-full py-2 mb-2 mb-lg-0">
      <li class="inline-block mr-1">
        <a href="https://twitter.com/share" class="twitter-share-button" data-hashtags="Decent Descent">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li class="inline-block">
        <a class="github-button" href="https://github.com//" data-icon="octicon-star" data-count-href="//stargazers" data-count-api="/repos//#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star / on GitHub">Star</a>
      </li>
    </ul>
  </div>
</div>
-->
<div class="clearfix mt-2 mt-lg-4">
  <div class="container mx-auto px-2">
  </div>
</div>

<!--
<script>
  $("script[type='math/tex']").replaceWith(function() {
      var tex = $(this).text();
      return katex.renderToString(tex, {displayMode: false});
  });

  $("script[type='math/tex; mode=display']").replaceWith(function() {
      var tex = $(this).html();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
  });
</script>
-->




  </body>

</html>