
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Multiple Variable Linear Regression</title>
  <script>(function(d) {var config = {kitId: 'all7jvn',scriptTimeout: 1000,async: true},h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='https://use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)})(document);</script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="https://lightning.svbtle.com/cargo/favicon-3686f5990e669ad2a1684f0ac250c4d9ddc03e2ef6f3c980f17b7aae786833ef.ico">
  <link rel="icon" sizes="196x196" href="https://lightning.svbtle.com/cargo/apple-touch-icon-8ed2bd858a30400ead0535543ffb8ad2ab3e036a2f0adb797dc641458d00a41a.png">
  <link rel="mask-icon" href="https://lightning.svbtle.com/cargo/default-b7e7b5361ab4c50a9ceb6dc296e0f157e2ec9c2f2c6f30832d991dc361d69512.svg" color="black">
  <!-- <meta name="generator" content="Svbtle.com" /> -->
  <meta name="description" content="C programming| Isack Odero "/>
  <link rel="canonical" href="https://oderoi.github.io/ulr" />
  <meta property="og:url" content="https://oderoi.github.io/ulr" />
  <meta property="twitter:site" content="@svbtle" />
  <meta property="twitter:title" content="Multiple Variable Linear Regression" />
  <meta property="twitter:description" content="Multiple Variable Linear Regression with C programming, ..." />
  <meta property="twitter:creator" content="@oderoi_" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:image" content="https://lightning.svbtle.com/cargo/icons/svbtle_logo-ba4b41e5249a7e3f19288fa586ea973569bfb83452114886be65bc5d8eb13b21.png" />
  <meta property="twitter:domain" content="https://oderoi.githu.io" />
  <meta property="og:title" content="Multiple Variable Linear Regression &bull; Isack Odero" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Multiple Variable Linear Regression using C programming language, ... | Isack Odero" />
  <meta property="og:image" content="https://lightning.svbtle.com/cargo/icons/svbtle_logo-ba4b41e5249a7e3f19288fa586ea973569bfb83452114886be65bc5d8eb13b21.png" />
  <meta property="og:site_name" content="Isack Odero" />
  <meta property="fb:app_id" content="346346195413177" />
  <link rel="alternate" type="application/rss+xml" href="https://oderoi.github.io/feed" />
  <link rel="stylesheet" href="https://lightning.svbtle.com/cargo/legacy/build.blog-120c367e4cc2cdf2d031c71f795ecea0ef4033f8b24d12d8e147c86e08e2ed2a.css" media="all" data-turbolinks-track="reload" />
  <script src="https://lightning.svbtle.com/cargo/build.blog-41a284c81b4230cd8ab812d35fabef8cc99e927407ed15604d3206997ef79818.js" data-turbolinks-track="reload"></script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VGMRYDBB5R"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('set', 'linker', {
      'accept_incoming': true
    });
    gtag('config', 'G-VGMRYDBB5R');
    gtag('config', 'UA-26652609-2');
  </script>  
  <script src="https://lightning.svbtle.com/cargo/share_buttons-dd547cdb8c37c1c5b949d9a5f034ef854d39634831852762b3d730f59a9e47d4.js" data-turbolinks-track="true"></script>
  <script src="//platform.twitter.com/widgets.js" async></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
 

<style>
  blockquote pre code {
    margin-left: -10px;
  }
@media screen and (max-width: 480px) {
  body pre code {
    font-size: 7px;
  }
}
@media screen and (min-width: 481px) and (max-width: 768px) {
  body pre code {
    font-size: 9px;
  }
}
@media screen and (min-width: 769px) and (max-width: 1024px) {
  body pre code {
    font-size: 11px;
  }
}
@media screen and (min-width: 1025px) {
  body pre code {
    font-size: 13px;
  }
}
</style>

</head>
<body class="overlord blog">
<style scoped>
figure#user_logo a,
figure#user_foot a,
figure.avatar a,
nav#overlord.user_top figure#logo_top a,
figure.kudo.complete div.filling {
  background-image: url('https://lightning.svbtle.com/cargo/blank-3dc89b51de1fd0e237e0320a05be98fb1f11cc04dcf200934ba8a64deec81ffd.png')
}

figure.kudo.activated div.filling,
figure.kudo.complete div.filling {
  background-color: #000000;
}

figure.kudo.activated a,
figure.kudo.complete a {
  border-color: #000000;
}

blockquote,
a blockquote,
div#readnext:hover span.flank_title,
div#foot_more:hover a,
div#foot_userbar a#bottom_tagline span:hover,
article.linked h1.article_title a:hover,
a.continue_button:hover,
article p a:hover,
ul#lightning_drop,
figure#user_foot,
ul#user_links li a:hover,
ul#foot_links li a:hover,
a.buttonize:hover,
button.buttonize:hover,
a.buttonize.outline:hover,
button.buttonize.outline:hover,
nav.pagination span.next a:hover,
nav.pagination span.prev a:hover,
section#readnext:hover p span,
nav#overlord.user_top figure#logo_top {
  border-color: #000000;
}
/*figure#user_logo,*/

figure.avatar,
nav#overlord.user_top figure#logo_top a,
ul#user_links li a:hover,
ul#foot_links li a:hover,
a.buttonize:hover,
button.buttonize:hover,
a.buttonize.outline:hover,
button.buttonize.outline:hover,
nav.pagination span.next a:hover,
nav.pagination span.prev a:hover,
figure#user_logo a,
figure#user_foot a  {
	background-color: #000000;
}

h6.separator_title.read_first,
header#user_top h2 a,
footer#blog_foot h5 a,
article.post h1 a:hover,
div.preview strong,
nav#overlord h2#nav_title.user_top a,
section#readnext:hover h3,
section#readnext:hover p span {
  color: #000000;
}

@keyframes titlePulse
  {
  0% {
    color: #000000;
  }
  50% {
    color: #000000;
  }
  100% {
    color: #000000;
  }
}

@-moz-keyframes titlePulse
  {
  0% {
    color: #000000;
  }
  50% {
    color: #000000;
  }
  100% {
    color: #000000;
  }
}

@-webkit-keyframes titlePulse
  {
  0% {
    color: #000000;
  }
  50% {
    color: #000000;
  }
  100% {
    color: #000000;
  }
}


</style>

<figure id="loading">&nbsp;</figure>
<nav id="overlord" class="user_top">
  <div id="lockup" class="">
    <figure id="logo_top" class=" user_top">
      <a href="/">Svbtle</a>
    </figure>
    <h2 id="nav_title" class="user_top"><a href="https://oderoi.github.io">Isack Odero</a></h2>
  </div>
  <figure id="hamburger">
    <a href="#menu" id="hamburger_button">Menu</a>
  </figure>
  <ul id="dropdown" class="onblog">
    <li><a href="https://x.com/oderoi_" class="xdotcom" target="_blank">@oderoi</a></li>
    <li style="margin: 0; padding: 0;"><hr class="overlord_nav" /></li>
    <li><a href="../about.html">about me</a></li>
  </ul>
</nav>

<section id="container" class="blog user_post">
  <article id="angbJyhsMUeJe4t4KyKHG1" class="post  historical">
    <header id="user_top" class="cf ">
      <ul id="user_links">
        <!-- <li><a href="https://x.com/oderoi_" class="xdotcom">@oderoi</a></li> -->
        <li><a href="https://github.com/oderoi/C/tree/main/Algoritm/Artificial%20Intelligence/Multiple%20variable%20linear%20regression" class="github" >oderoi</a></li>
        <li><a href="../about.html">About me</a></li>
      </ul>
    </header>
    
	<time datetime="2016-05-03" class="article_time">May  3, 2016</time>
  <h1 class="article_title">
    <a href="./mlr.html">Multiple Variable Linear Regression</a>
  </h1>
	<h1 id="problem_statement_1">Problem statement <a class="head_anchor" href="#problem_statement_1">#</a></h1>
  <p>We will use the motivating example of housing price prediction. The training dataset contains four features (size, bedrooms, floors and, age) shown in the table below.</p>
  

  <table>
    <tr>
      <th>Size (sqft)</th>
      <th>Number of Bedrooms </th>
      <th>Number of floors</th>
      <th>Age of home (years)</th>
      <th>Price (1,000s dollars)</th>
      
  </tr>
    <tr>
      <td>2104</td>
      <td>5</td>
      <td>1</td>
      <td>45</td>
      <td>460</td>
    </tr>
    <tr>
      <td>1416</td>
      <td>3</td>
      <td>2</td>
      <td>40</td>
      <td>232</td>
    </tr>
    <tr>
      <td>852</td>
      <td>2</td>
      <td>1</td>
      <td>35</td>
      <td>178</td>
    </tr>
  </table>
  <p>Let's build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  </p>

  <h2 id="problem_statement_1">Matrix \(X\) containing our example <a class="head_anchor" href="#problem_statement_1">#</a></h2>
  <p>Each row of datasets represent one example, with \(n\) number of features and \(m\) training examples, now \(\mathbf{x}\) is an input matrix with dimensions \((m, n)\) where \(m\) is row and $n$ is column.</p>
  <p>$$\mathbf{X} = 
    \begin{pmatrix}
     x^{(0)}_0 & x^{(0)}_1 & \cdots & x^{(0)}_{n-1} \\ 
     x^{(1)}_0 & x^{(1)}_1 & \cdots & x^{(1)}_{n-1} \\
     \vdots  &  \vdots  &  \vdots  &  \vdots  \\
     x^{(m-1)}_0 & x^{(m-1)}_1 & \cdots & x^{(m-1)}_{n-1} 
    \end{pmatrix}
    $$</p>

<p>Notations</p>
<ul>
  <li>\(\mathbf{x}^{(i)}\) is vector containing example i. \(\mathbf{x}^{(i)} = (x^{(i)}_0, x^{(i)}_1, \cdots,x^{(i)}_{n-1})\)</li>
  <li>\(x^{(i)}_j\) is element \(j\) in example \(i\). The superscript in parenthesis indicates the example number while the subscript represents an element.</li>
</ul>

<h2 id="problem_statement_1">Parameter vector \(\mathbf{w}_{j}\) , \(b\) <a class="head_anchor" href="#problem_statement_1">#</a></h2>
<ul>
  <li>
    \(\mathbf{w}\) is a vector with \(n\) elements.
    <ul>
      <li>Each element contains the parameter associated with one feature.</li>
      <li>in our dataset, n is 4.</li>
      <li>notionally, we draw this as a column vector</li>
      <p>$$\mathbf{w} = \begin{pmatrix}
        w_0 \\ 
        w_1 \\
        \vdots\\
        w_{n-1}
        \end{pmatrix}
        $$</p>
    </ul>
  </li>
  <li>\(b\) is a scalar parameter. </li>
</ul>


<h1 id="load_data_1">Load datasets <a class="head_anchor" href="#load_data_1">#</a></h1>
<p>As shown in a problem statement, our dataset contains five features (size, bedrooms, floors and, age, price of the house), \(n = 5\) and ahundred examples, \(m = 100\).</p>
<p>In this case our dataset will have the size of \((m , n) = (100 , 5)\)</p>
<p>
  $$\mathbf{data} = 
\begin{pmatrix}
 x^{(0)}_{0} & x^{(0)}_{1} & x^{(0)}_{2} & \cdots & y^{(0)}_{n-1} \\ 
 x^{(1)}_{0} & x^{(1)}_{1} & x^{(1)}_{2} & \cdots & y^{(1)}_{n-1} \\
 \vdots      & \vdots      & \vdots      & \vdots      & \vdots\\
 x^{(m-1)}_{0} & x^{(m-1)}_{1} & x^{(m-1)}_{2} & \cdots & y^{(m-1)}_{n-1}
\end{pmatrix}

=
 
\begin{pmatrix}
 x^{(0)}_{0} & x^{(0)}_{1} & x^{(0)}_{2} & x^{(0)}_{3} & y^{(0)}_{4} \\ 
 x^{(1)}_{0} & x^{(1)}_{1} & x^{(1)}_{2} & x^{(1)}_{3} & y^{(1)}_{4} \\
 \vdots      & \vdots      & \vdots      & \vdots      & \vdots\\
 x^{(99)}_{0} & x^{(99)}_{1} & x^{(99)}_{2} & x^{(99)}_{3} & y^{(99)}_{4}
\end{pmatrix}
$$
</p>

<p>
  $$data \in \mathbb{R}_{m \times n}  =  \mathbb{R}_{100 \times 5}$$
</p>


<h2 id="load_data_1">Load input datasets, \(\mathbf{x}^{(i)}_{j}\) <a class="head_anchor" href="#load_data_1">#</a></h2>

<p>Input datasets will comprise of five features (size, bedrooms, floors and, age), \(n = 4\) and hundred exaples, \(m = 100\).</p>
<p>
  $$\mathbf{X} = 
\begin{pmatrix}
 x^{(0)}_{0} & x^{(0)}_{1} & \cdots & x^{(0)}_{n-1}  \\ 
 x^{(1)}_{0} & x^{(1)}_{1} & \cdots & x^{(1)}_{n-1}  \\
 \vdots      & \vdots      & \vdots      & \vdots \\
 x^{(m-1)}_{0} & x^{(m-1)}_{1} & \cdots & x^{(m-1)}_{n-1} 
\end{pmatrix}
 = 
\begin{pmatrix}
 x^{(0)}_{0} & x^{(0)}_{1} & x^{(0)}_{2} & x^{(0)}_{3}  \\ 
 x^{(1)}_{0} & x^{(1)}_{1} & x^{(1)}_{2} & x^{(1)}_{3}  \\
 \vdots      & \vdots      & \vdots      & \vdots \\
 x^{(99)}_{0} & x^{(99)}_{1} & x^{(99)}_{2} & x^{(99)}_{3} 
\end{pmatrix}
$$
</p>
  $$\mathbf{X} \in \mathbb{R}_{m \times n}  =  \mathbb{R}_{100 \times 4}$$
<p>

<h2 id="load_data_1">Load output datasets, \(\mathbf{y}^{(i)}_{j}\) <a class="head_anchor" href="#load_data_1">#</a></h2>

<p>
  Input datasets will comprise of only one features (price), $n = 1$ and hundred exaples, $m = 100$.
</p>

<p>
  $$\mathbf{Y} = 
\begin{pmatrix}
  y^{(0)}_{0} \\ 
  y^{(1)}_{0} \\
  \vdots     \\
 y^{(m-1)}_{0}
\end{pmatrix}
 = 
\begin{pmatrix}
  y^{(0)}_{0} \\ 
  y^{(1)}_{0} \\
  \vdots     \\
 y^{(99)}_{0}
\end{pmatrix}
$$
</p>

<p>
  $$Y \in \mathbb{R}_{m \times n}  =  \mathbb{R}_{100 \times 1}$$
</p>

<h2 id="load_data_1">Initialize parameters  \(\mathbf{w}_{j}\)  , b<a class="head_anchor" href="#load_data_1">#</a></h2>
<ul>
  <li>
    \(\mathbf{w}_{j}\) is a vector with $n = 4$ elements.
    <ul>
      <li>
        Each element contains the parameter associated with one feature.
      </li>
      <p>
        $$\mathbf{w} = \begin{pmatrix}
        w_{0} \\ 
        w_{1} \\
        \vdots\\
        w_{n-1}
        \end{pmatrix}
        = \begin{pmatrix}
        w_{0} \\ 
        w_{1} \\
        w_{2}\\
        w_{3}
        \end{pmatrix}
        $$
      </p>
      <p>
        $$\mathbf{w} \in \mathbb{R}_{n \times 1}  =  \mathbb{R}_{4 \times 1}$$
      </p>
    </ul>
  </li>
  <li>
    \(b\) is a scalar parameter.
  </li>
</ul>


<h1 id="gradient_descent_1">Model Prediction With Multiple Variables <a class="head_anchor" href="#gradient_descent_1">#</a></h1>
<p>The model's prediction with multiple variables is given by the linear model:</p>

<p>$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \tag{1}$$</p>
<p>or in vector Notations</p>
<p>
  $$ f_{\mathbf{w},b}(\mathbf{x}) = \mathbf{w} \cdot \mathbf{X} + \mathbf{b}  \tag{2} $$ 
</p>
<p>
  $$f_{\mathbf{w},b}(\mathbf{x}) \in \mathbb{R}_{m \times 1}  =  \mathbf{w} \in \mathbb{R}_{n \times 1} \cdot \mathbf{X} \in \mathbb{R}_{m \times n} + \mathbf{b} \in \mathbb{R}_{1}$$
</p>
<p>
  $$f_{\mathbf{w},b}(\mathbf{x})_{m \times 1}  =  \mathbf{w}_{n \times 1} \cdot \mathbf{X}_{m \times n} + \mathbf{b}_{1}$$
</p>

<p>
  $$f_{\mathbf{w},b}(\mathbf{x}^{(i)}) \in \mathbb{R}_{m \times 1}$$
</p>
<p>
  $$f_{\mathbf{w},b}(\mathbf{x}) = 
  \begin{pmatrix}
    f_{\mathbf{w},b}(\mathbf{x})^{(0)}_{0} \\ 
    f_{\mathbf{w},b}(\mathbf{x})^{(1)}_{0} \\
    \vdots     \\
  f_{\mathbf{w},b}(\mathbf{x})^{(m-1)}_{0}
  \end{pmatrix}
  = 
  \begin{pmatrix}
    f_{\mathbf{w},b}(\mathbf{x})^{(0)}_{0} \\ 
    f_{\mathbf{w},b}(\mathbf{x})^{(1)}_{0} \\
    \vdots     \\
  f_{\mathbf{w},b}(\mathbf{x})^{(99)}_{0}
  \end{pmatrix}
  $$
</p>
<p>where \(\cdot\) is a vector `dot product`</p>
<p>To demonstrate the dot product, we will implement prediction using (1) and (2).</p>

<blockquote>
<pre>
<code>
double *predict(float **x, double w[][1], double b){

  /*
  single predict using linear regression
  
  Args:
    x (ndarray): Shape (m, n) example with multiple features
    w (ndarray): Shape (n, 1) model parameters    
    b (scalar):  model parameter     
    
  Returns:
    p (scalar):  prediction
  
  */

  double p;
  double *f_wb=(double *)malloc(m*sizeof(double));
  if (f_wb==NULL)
  {
    perror("Error allocating memory");
    free(f_wb);
  }
  

  for (int i = 0; i < m; i++)
  {
    for (int j = 0; j < 1; j++)
    {   
      p=0;
      for (int k = 0; k < n; k++)
      {
          p += (double)x[i][k]*w[k][j];
      }
      p += b;
      f_wb[i]=p;
        
    } 
  }
  
  return f_wb;
}
</code>
</pre>
</blockquote>

<h1 id="cost_1">Compute Cost With Multiple Variables <a class="head_anchor" href="#cost_1">#</a></h1>

<p>The equation for the cost function with multiple variables \(J(\mathbf{w},b)\) is:</p>
<p>
  $$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 \tag{3}$$ 
where:
$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{X} + b  \tag{4} $$ 
</p>
<p>
  $$f_{\mathbf{w},b}(\mathbf{x}) \in \mathbb{R}_{m \times 1}$$
</p>
<p>
  In contrast to previous labs, \(\mathbf{w}\) and \(\mathbf{x}^{(i)}\) are vectors rather than scalars supporting multiple features.
</p>

<blockquote>
<pre>
<code>
double compute_cost(float **x, float y[m], double w[][1], double b){
  
  /*
    compute cost
  Args:
    X (ndarray (m,n)): Data, m examples with n features
    y (ndarray (m,1)) : target values
    w (ndarray (n,1)) : model parameters  
    b (scalar)       : model parameter
    
  Returns:
    cost (scalar): cost
  
  */

  double cost=0;
  double p;

  for (int i = 0; i < m; i++)
  {
    for (int j = 0; j < 1; j++)
    {
      p=0;
      for (int k = 0; k < n; k++)
      {
        p += (double)x[i][k]*w[k][j];
      }
      p +=b;
    }
    cost += pow((p - (double) y[i]), 2);
  }
  cost /= (2*m);

  return cost;
}
</code>
</pre>
</blockquote>

<h1 id="learning_parameter_1">Gradient Descent With Multiple Variables<a class="head_anchor" href="#learning_parameter_1">#</a></h1>

<p>Gradient descent for multiple variables:</p>
<p>
  $$\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \newline\;
& w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{5}  \; & \text{for j = 0..n-1}\newline
&b\ \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline \rbrace
\end{align*}$$
</p>
<p>
  where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  
</p>
<p>
  $$
  \begin{align}
  \frac{\partial J(\mathbf{w},b)}{\partial w_j}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \tag{6}  \\
  \frac{\partial J(\mathbf{w},b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{7}
  \end{align}
  $$
</p>
<ul>
  <li>m is the number of training examples in the data set</li>
  <li>\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\) is the model's prediction, while \(y^{(i)}\) is the target value</li>
</ul>

<p>Note</p>
<ul>
  <li>
    <p>
      $$\frac{\partial J(\mathbf{w},b)}{\partial w_j} \in \mathbb{R}_{1 \times n} = \mathbb{R}_{1 \times 4}$$
    </p>
    <p>
      $$\frac{\partial J(\mathbf{w},b)}{\partial w_j}
      = 
      \begin{pmatrix}
        \frac{\partial J(\mathbf{w},b)}{\partial w_0} ,
        \frac{\partial J(\mathbf{w},b)}{\partial w_1} ,
        \cdots ,
      \frac{\partial J(\mathbf{w},b)}{\partial w_{n-1}}
      \end{pmatrix}
      = 
      \begin{pmatrix}
        \frac{\partial J(\mathbf{w},b)}{\partial w_0} ,
        \frac{\partial J(\mathbf{w},b)}{\partial w_1} ,
        \frac{\partial J(\mathbf{w},b)}{\partial w_2} ,
      \frac{\partial J(\mathbf{w},b)}{\partial w_3}
      \end{pmatrix}
      $$
    </p>
  </li>
  <li>
    $$\frac{\partial J(\mathbf{w},b)}{\partial b} \in \mathbb{R}_{1}$$
  </li>
</ul>
<blockquote>
<pre>
<code>
double *compute_gradient(float **x, float y[m], double w[][1], double b){
  /*

  Computes the gradient for linear regression 
  Args:
    X (ndarray (m,n)): Data, m examples with n features
    y (ndarray (m,1)) : target values
    w (ndarray (n,1)) : model parameters  
    b (scalar)       : model parameter
    
  Returns:
    dj_dw (ndarray (n,1)): The gradient of the cost w.r.t. the parameters w. 
    dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. 

    grads (ndarray ((n+1), 1)): dj_dw and dj_db. 
  */

  double *grads=(double *)malloc((n + 1)*sizeof(double));
  if (grads ==NULL)
  {
      perror("Error in allocating memory");
      free(grads);
  }

  double *f_wb=(double *)malloc(m*sizeof(double));
  if (f_wb ==NULL)
  {
      perror("Error in allocating memory");
      free(f_wb);
  }
  
  double p;
  double dj_dw;
  double dj_db=0;
  
  for (int i = 0; i < m; i++)
  {
    for (int j = 0; j < 1; j++)
    {
      p=0;
      for (int k = 0; k < n; k++)
      {
        p += x[i][k]*w[k][j];
      }
      p += b;
      f_wb[i]=p;
    }

    dj_db += f_wb[i] - y[i];
  }
  dj_db /= m;


  for (int i = 0; i < n; i++)
  {
    dj_dw=0;
    for (int j = 0; j < m; j++)
    {
      dj_dw += (f_wb[j] - y[j])*x[j][i];
    }
    dj_dw /= m;

    grads[i]=dj_dw;       
  }

  grads[n]=dj_db;

  free(f_wb);

  return grads;
}
</code>
</pre>
</blockquote>

<h1 id="Evaluating_1">Evaluating our model by compute R-squared<a class="head_anchor" href="#Evaluating_1">#</a></h1>

<p>R-squared is the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. It ranges from 0 to 1, where 0 means no relationship and 1 means a perfect fit</p>
<p>R-squared(coefficient of Determination) tells us How well our model is  performing or how well our model's predictions match the real results. A higher R-squared means our model is doing a better job predicting.</p>
<p>
  $$
  R^2 = 1 - \frac{Residual\ Sum\ of\ Squares\ (\mathbf{ss}_{res})}{Total\ Sum\ of\ Squares\ (\mathbf{ss}_{total})}
  $$
</p>
<p>
  <img src="./images/ssr.png" >
</p>
<p>
  <img src="./images/sst.png" >
</p>
<p>
  $$
  R^2 = 1 - \frac{\mathbf{ss}_{res}}{\mathbf{ss}_{total}}\\
  $$
</p>
<p>
  $$
  R^2 = 1 - \frac{\sum\limits_{i=0}^{(m-1)}(f_{w,b}(x^{(i)}) - y^{i})^2}{\sum\limits_{i=0}^{(m-1)}(y_{mean}^{(i)} - y^{(i)})^2}
  $$
</p>

<blockquote>
<pre>
<code>
double r_squared(float **x, float y[m], double w[][1], double b){

  /*

  Computes the gradient for linear regression 
  Args:
    X (ndarray (m,n)): Data, m examples with n features
    y (ndarray (m,1)) : target values
    w (ndarray (n,1)) : model parameters  
    b (scalar)       : model parameter
    
  Returns:
    r_square (scalar): r square score.
    r_square = (1 - rss/tss) 
    rss : measure of sum diff btn actual(y) value and predicted(f_wb) value.(y - f_wb)
    tss : measure of total variance in actual(y) value. (y - f_mean)
  
  */

  double r_square;
  double f_wb;
  double y_mean;
  double y_sum;
  double rss;
  double tss;

  for (int i = 0; i < m; i++)
  {
    for (int j = 0; j < 1; j++)
    {
      f_wb=0;
      for (int k = 0; k < n; k++)
      {
        f_wb += x[i][k]*w[k][j];
      }
      f_wb +=b;
    }

    rss += pow((y[i] - f_wb), 2);
    y_sum += y[i];
      
  }
  y_mean = y_sum/m;

  for (int i = 0; i < m; i++)
  {
    tss += pow((y[i] - y_mean), 2);
  }
  
  r_square = 1 - (rss/tss);


  return r_square;
}
</code>
</pre>
</blockquote>

<!-- <h1 id="main_1">The main function<a class="head_anchor" href="#main_1">#</a></h1>
<blockquote>
<pre>
<code>

</code>
</pre>
</blockquote> -->

  <!-- <figure class="postend kudo able clearfix" id="kudo_angbJyhsMUeJe4t4KyKHG1">
    <a href="#kudo">
      <div class="filling">&nbsp;</div>
    </a>
    <div class="num">2,076</div>
    <div class="txt">Kudos</div>
  </figure>
  <figure class="side kudo able clearfix" id="kudo_side_angbJyhsMUeJe4t4KyKHG1">
    <a href="#kudo">
      <div class="filling">&nbsp;</div>
    </a>
    <div class="num">2,076</div>
    <div class="txt">Kudos</div>
  </figure> -->
</article>

  <div id="share_links" data-no-turbolink>
    <a href="https://twitter.com/share" class="twitter-share-button" data-via="oderoi" data-related="svbtle" data-no-turbolink>Tweet</a>
    <!-- <div style="margin-top: 4px; margin-bottom: 8px; margin-left: 0px; display: block;" class="fb-share-button" data-href="https://oderoi.github.io/Univariate-linear-regression" data-layout="button_count" data-no-turbolink></div> -->
  <div>
</section>
<section id="readnext">
  <a href="../bsearch/bsearch.html">
    <h4 class="readnext_header">Now read this</h4>
    <h3 class="readnext_title">Factorial</h3>
    <p class="readnext_content">C programming language,... <span class="continue_btn">Continue&nbsp;&rarr;</span></p>
  </a>
</section>
<footer id="blog_foot" class="cf">
  <ul id="foot_links">
    <li><a href="https://x.com/oderoi_">@oderoi</a></li>
    <li><a href="https://oderoi.github.io" >oderoi</a></li>
  </ul>
  <figure id="user_foot"><a href="/">Svbtle</a></figure>
  <h5><a href="https://oderoi.github.io">Isack Odero</a></h5>
</footer>


<div id="lights">&nbsp;</div>
<div id="app-data" data-name="svbtle" data-version="8.5-legible" data-magicNum="2572031820.15"></div><div id="px-data" data-ax="posts" data-sx="show"></div><div id="user-data" data-here="false" data-state="logged-out"></div><div id="blog-data" data-title="Greg Brockman" data-blogname="gdb" data-extid="8SMTiky8WK9x16AjSma1Vx" data-color="000000" data-color-rgb="0,0,0" data-color-rgba="(0,0,0,0.5)" data-blog-tracker="UA-26652609-2"></div></body>
</html>